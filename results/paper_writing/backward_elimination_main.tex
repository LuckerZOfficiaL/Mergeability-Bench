\paragraph{Backward Elimination.}
As a complementary approach to L1 regularization, we explore backward elimination (reverse greedy selection) to identify dispensable metrics. Starting from all 29 metrics, we iteratively remove the metric whose elimination causes the smallest decrease in training correlation, stopping when any removal would decrease performance by more than $\tau = 0.001$. Unlike L1 regularization which jointly sparsifies coefficients, backward elimination directly measures each metric's marginal contribution. This approach yields a surprising result: removing 2--5 metrics per fold (retaining $\sim$26 on average) \emph{improves} validation correlation by $+0.044$ on average (Table~\ref{tab:backward_elimination}). The most frequently removed metrics include \texttt{layerwise\_effective\_rank} (17.5\%), \texttt{task\_vector\_cosine\_similarity} (16.2\%), and \texttt{singular\_value\_ratio} (16.2\%)---metrics with high redundancy or noise. In contrast, \texttt{interaction\_matrix\_overlap\_top\_k} and \texttt{input\_gradient\_l2\_distance} are retained in 100\% of folds, confirming their unique predictive value. This suggests the original 29-metric set contains mildly detrimental metrics that harm generalization. See Appendix~\ref{app:backward_elimination} for full results.
