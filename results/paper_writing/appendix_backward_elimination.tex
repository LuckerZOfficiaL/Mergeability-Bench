\section{Backward Elimination for Metric Selection}
\label{app:backward_elimination}

We perform backward elimination (reverse greedy selection) to identify which metrics can be removed without harming---or potentially improving---prediction performance. This complements the L1 regularization approach by directly measuring each metric's marginal contribution.

\subsection{Method}

Starting from the full set of 29 metrics, we iteratively:
\begin{enumerate}
    \item For each remaining metric, compute the training correlation if that metric were removed
    \item Remove the metric whose elimination causes the smallest decrease (or largest increase) in training correlation
    \item Stop when removing any metric would decrease training correlation by more than threshold $\tau$
\end{enumerate}

We use $\tau = 0.001$ and perform this selection independently for each LOTO fold using only training data. Validation data is used solely for final evaluation (no leakage).

\subsection{Results}

\begin{table}[htbp]
\centering
\caption{Backward elimination results compared to baseline (all 29 metrics) and forward greedy selection. Val $r$ reported as mean $\pm$ std across 20 LOTO folds. $N$ indicates average number of metrics retained/selected.}
\label{tab:backward_elimination}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Baseline} & \textbf{Forward Greedy} & \textbf{Backward Elim.} & \textbf{Fwd $\Delta$} & \textbf{Bwd $\Delta$} \\
& ($N$=29) & ($N$$\approx$11) & ($N$$\approx$26) & & \\
\midrule
Weight Avg & 0.555 & 0.517 & \textbf{0.583} & $-$0.038 & $+$0.028 \\
Arithmetic & 0.343 & 0.229 & \textbf{0.406} & $-$0.114 & $+$0.062 \\
TSV & 0.572 & 0.433 & \textbf{0.602} & $-$0.139 & $+$0.030 \\
Isotropic & 0.328 & 0.336 & \textbf{0.385} & $+$0.007 & $+$0.057 \\
\midrule
\textbf{Average} & 0.449 & 0.379 & \textbf{0.494} & $-$0.071 & $+$0.044 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Finding: Removing Metrics Improves Generalization.}
Backward elimination consistently outperforms the baseline across all four merging methods, with an average improvement of $+0.044$ in validation correlation. This indicates that the original 29-metric set contains metrics that hurt generalization---likely due to overfitting to training data patterns or introducing noise.

\paragraph{Contrast with Forward Greedy Selection.}
Forward greedy selection (starting from empty set, adding metrics) performs substantially worse ($-0.071$ on average), despite selecting a similar number of ``important'' metrics. This asymmetry suggests that:
\begin{itemize}
    \item The predictive information is distributed across many metrics, making it difficult to capture with a small subset
    \item A few metrics actively harm performance and should be removed rather than others being added
\end{itemize}

\subsection{Metric Retention Analysis}

\begin{table}[htbp]
\centering
\caption{Metric retention frequency across all methods and folds. Retention rate indicates the proportion of folds where the metric was kept (not removed).}
\label{tab:retention_frequency}
\small
\begin{tabular}{lc|lc}
\toprule
\multicolumn{2}{c|}{\textbf{Most Frequently Retained}} & \multicolumn{2}{c}{\textbf{Most Frequently Removed}} \\
\textbf{Metric} & \textbf{Rate} & \textbf{Metric} & \textbf{Rate} \\
\midrule
interaction\_matrix\_overlap\_top\_k & 100.0\% & layerwise\_effective\_rank & 17.5\% \\
input\_gradient\_l2\_distance & 100.0\% & task\_vector\_cosine\_similarity & 16.2\% \\
activation\_cosine\_similarity & 98.8\% & singular\_value\_ratio & 16.2\% \\
right\_subspace\_overlap\_bottom\_k & 97.5\% & task\_vector\_magnitude\_ratio & 15.0\% \\
interaction\_matrix\_overlap\_bottom\_k & 97.5\% & subspace\_overlap & 15.0\% \\
layerwise\_effective\_rank\_ms & 97.5\% & right\_subspace\_overlap\_top\_k & 15.0\% \\
task\_vector\_l2\_distance & 96.2\% & activation\_magnitude\_ratio & 13.8\% \\
stable\_rank & 95.0\% & task\_vector\_dot\_product & 13.8\% \\
activation\_dot\_product & 95.0\% & effective\_rank\_mergeability\_score & 12.5\% \\
effective\_rank & 93.8\% & encoder\_gradient\_dot\_product & 11.2\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Per-Method Removal Patterns}

\begin{table}[htbp]
\centering
\caption{Top 5 most frequently removed metrics per merging method (removal rate in parentheses).}
\label{tab:per_method_removal}
\small
\begin{tabular}{ll}
\toprule
\textbf{Method} & \textbf{Most Frequently Removed Metrics} \\
\midrule
Weight Avg & layerwise\_effective\_rank (25\%), singular\_value\_ratio (20\%), \\
& task\_vector\_magnitude\_ratio (20\%), right\_subspace\_overlap\_top\_k (15\%) \\
\midrule
Arithmetic & task\_vector\_magnitude\_ratio (20\%), singular\_value\_ratio (15\%), \\
& layerwise\_effective\_rank (15\%), task\_vector\_cosine\_similarity (15\%) \\
\midrule
TSV & right\_subspace\_overlap\_top\_k (40\%), task\_vector\_cosine\_similarity (40\%), \\
& singular\_value\_overlap (40\%), effective\_rank\_mergeability\_score (35\%) \\
\midrule
Isotropic & task\_vector\_dot\_product (15\%), activation\_magnitude\_ratio (15\%), \\
& activation\_dot\_product (15\%), encoder\_gradient\_l2\_distance (15\%) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

\paragraph{Why Do Some Metrics Hurt Performance?}
The most frequently removed metrics fall into two categories:
\begin{itemize}
    \item \textbf{Redundant metrics}: \texttt{task\_vector\_cosine\_similarity}, \texttt{task\_vector\_magnitude\_ratio}, and \texttt{singular\_value\_ratio} capture information already present in other metrics (e.g., subspace overlaps). Their inclusion adds noise without new signal.
    \item \textbf{Unstable metrics}: \texttt{layerwise\_effective\_rank} aggregates rank information across layers, but this aggregation may obscure layer-specific patterns important for mergeability. The layerwise \emph{mergeability score} variant, which normalizes appropriately, is retained at 97.5\%.
\end{itemize}

\paragraph{Core Predictive Metrics.}
The 100\% retained metrics---\texttt{interaction\_matrix\_overlap\_top\_k} and \texttt{input\_gradient\_l2\_distance}---represent two complementary views of mergeability:
\begin{itemize}
    \item \texttt{interaction\_matrix\_overlap\_top\_k} captures structural alignment of how models transform inputs
    \item \texttt{input\_gradient\_l2\_distance} captures functional similarity via gradient responses
\end{itemize}

\paragraph{Implications for Metric Design.}
These results suggest that future mergeability metrics should focus on:
\begin{enumerate}
    \item Interaction-based measures that capture input-output relationships
    \item Gradient-based measures computed on calibration data
    \item Properly normalized aggregations (e.g., mergeability scores rather than raw effective ranks)
\end{enumerate}

Simple geometric measures in weight space (cosine similarity, magnitude ratios) appear less informative than structured measures of how models process data.

\subsection{Comparison Summary}

\begin{table}[htbp]
\centering
\caption{Summary comparison of metric selection strategies.}
\label{tab:selection_comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Strategy} & \textbf{Avg Val $r$} & \textbf{$\Delta$ vs Baseline} & \textbf{Avg $N$ Metrics} \\
\midrule
Baseline (all metrics) & 0.449 & --- & 29 \\
L1 Regularization ($\lambda$=0.1) & 0.453 & $+$0.004 & 29 (sparse) \\
Forward Greedy ($\tau$=0.001) & 0.379 & $-$0.071 & 10.9 \\
\textbf{Backward Elimination} ($\tau$=0.001) & \textbf{0.494} & $+$0.044 & 26.4 \\
\bottomrule
\end{tabular}
\end{table}

Backward elimination emerges as the most effective strategy, improving over both the baseline and L1 regularization while maintaining interpretability about which metrics are dispensable.
